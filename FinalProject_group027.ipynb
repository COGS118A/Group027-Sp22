{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Amazon Reviews\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Amy Hardy\n",
    "- Dina Dehaini\n",
    "- Stephanie Kwan\n",
    "- Darren Wu\n",
    "- Nicholas Hale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Sentiment analysis can help business owners process huge amounts of feedback from reviewers and automatically tag them. In our project, we will implement sentiment analysis on over 500k Amazon reviews and attempt to accurately tag/predict reviews based on the actual text of the review. Labels will be derived from their ratings with ratings of 4-5 being positive, 3 neutral, and 1-2 negative. We will also attempt the regression task of predicting a rating between the continuous range of 1 and 5 stars using the review.  We will preprocess the text data through the typical means of removing stop words, stemming, lemmatization, tokenization, and vectorization. Next, an SVM, random forest classifier, and logistic regression model will be trained on the extracted features and evaluated on the remaining data with performance being measured through each model's accuracy in predicting both the labels and the ratings.\n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Sentiment Analysis is a powerful method for corporations and other organizations to analyze text for a variety of purposes. Sentiment analysis can be defined as the procedure of utilizing natural language processing, text analysis, and other metrics to determine the tonality of a body of text.<a name=\"one\"></a>[<sup>[1]</sup>](#one) When utilized normally, the polarity of analyzed text is separated into categorizations, such as ‘Positive’, ‘Negative’, and ‘Neutral’<a name=\"four\"></a>[<sup>[4]</sup>](#four).\n",
    "\n",
    "The specific manners in which sentiment analysis can be utilized varies along a great range.\tSentiment analysis allows for a consistent metric to analyze text, and in the context of something such as product reviews, allows for the removal of unconscious bias in its analysis in addition to allowing for greater amounts of data to be processed. In a corporate context, sentiment analysis can be utilized to gain realtime insights into consumer opinions and satisfaction and allow for trends to be identified by a text’s polarity.\n",
    "\n",
    "In the context of machine learning, there are a number of algorithms that can be used to analyze text, from Naive Bayes, Linear Regression, Support Vector Machines, neural networks, and more.<a name=\"two\"></a>[<sup>[2]</sup>](#two)\n",
    "\n",
    "Each model has challenges with their implementations, and there are general challenges with sentiment analysis itself in its current state. Text is very nuanced, and there are certain aspects of tone, sentence structure, slang, subjectivity, and more that cause for necessity of improvements upon current models to allow this tool to improve in accuracy.<a name=\"three\"></a>[<sup>[3]</sup>](#three) With further research into this topic, it is possible to identify specific algorithms that allow for the most accuracy when performing sentiment analysis. Performing sentiment analysis on the same dataset multiple times while varying the models used will allow for one to identify which model provides the greatest accuracy for this specific type of data. Product reviews invite different language use by those submitting them than other forms of text, and as mentioned before, performing sentiment analysis on this form of data is largely beneficial to corporations and other parties that may be interested in consumer feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we will be tackling in this project is how to best classify reviews in terms of positive, neutral, and negative as well as in terms of rating. We want to compare how these two different kinds of outputs will perform, and to see which method will produce a better accuracy. We will implement an SVM, random forest classifier, and logistic regression based sentiment analysis model. Specifically, each model will take as input several training reviews and their ratings and then test its accuracy against several test reviews and their ratings by classifying the polarity of a given Amazon review and automatically tag it as positive, neutral, or negative, and also see if we can then give the review a rating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The dataset can be found at https://www.kaggle.com/datasets/jillanisofttech/amazon-product-reviews.\n",
    " \n",
    "This dataset has a total of 568k customer reviews. Each review includes the following fields:\n",
    "- ProductID: the id of the Amazon product\n",
    "- UserID: the id of the Amazon user who is reviewing the product\n",
    "- ProfileName: the profile name of the user who is reviewing the product\n",
    "- HelpfulnessNumerator: number of people who found the review helpful\n",
    "- HelpfulnessDenominator: number of people who did not find the review helpful\n",
    "- Score: the rating the user is giving the product, on a scale of 1-5 stars\n",
    "- Time: the timestamp that the review was posted at\n",
    "- Summary: a short phrase that summarizes the review\n",
    "- Text: the actual text of the review\n",
    "\n",
    "For our project, we will be dropping most features besides the variables that contain text (customer reviews/summary) and rating. The UserID and ProfileName will provide no help in classifying a review’s sentiment, and neither will the timestamp. The values in the helpfulness numerator and denominator often match in each row, which makes their usage and meaning confusing, and overall we believe that the fields will not provide our model additional useful information. We plan to drop the summary field initially as well.\n",
    "\n",
    "An additional column for classification will be made based on the score column –- numbers 1-2 will be -1 for “negative”, 3 will be 0 for “neutral”, and 4-5 will be 1 for “positive” in the new classification column. The reviews and summary will undergo text preprocessing and vectorization before their use in training the models.\n",
    "\n",
    "## Cleaned Data\n",
    "Due to the size of the original CSV file being ~300MB, we had to split it into 4 separate, smaller CSV files so that we could use GitHub. We then wrote a function to retrieve and merge these files together and return the completed dataset.\n",
    "\n",
    "## EDA\n",
    "\n",
    "Taking a closer look at the data, we can see it's heavily skewed and we have a disportionate number of positive reviews. This may impact our model's ability to accurately predict negative review or neutral reviews. This in turn could lead to a higher true positive rate but also a higher false positive rate meaning we'd have a more sensitive and specific model. \n",
    "\n",
    "We also used wordclouds to look at some of the more frequent words based on their sentiment label for both the \"Review\" and \"Summary\" columns. From these plots we can see another potential issue: our model may start to associate neutral words with positive or negative sentiments. Because the products these reviews come from have a star rating and many of the reviews come from these same products, we are left with many reviews for the same product in the same sentiment category. Of course many of these reviews also mention the name of the product (i.e. dog food) and so these words become the most frequent in this sentiment category thus creating an issue for us. Unfortunately, product names were not included as a feature so dropping them from the reviews or summaries may not be possible. Looking at the wordclouds for the summaries, we notice slightly better results in that frequent words are not as neutral as words from the reviews themselves. There are still product names especially in the negative summaries but overall, models trained on the summaries may be able to generalize a little better than models trained on the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Before feeding the data to any of our models, we will preprocess and vectorize our dataset using scikit-learn's CountVectorizer and TfidfVectorizer classes.\n",
    "\n",
    "**Support Vector Machine**: SVM is a supervised learning algorithm that can be used for both classification and regression. The algorithm is based on identifying a hyperplane that best separates the data into classes and maximizes the separation boundaries between the data points.<a name=\"five\"></a>[<sup>[5]</sup>](#five) The hyperplane is constructed by transforming the data with kernels, which can be linear, sigmoid, polynomial, etc. Because SVM in its simplest form supports binary classification, we will be implementing multiclass SVM (either one vs. one, one vs. all, or directed acyclic graph approach). These involve breaking down the classification into multiple binary classification cases. We plan to use scikit-learn's SVM class in our implementation.\n",
    "\n",
    "**Random Forest Classifier**: RFC is a supervised learning algorithm that can be used for both classification and regression. The algorithm is based on creating several decision trees, which helps in preventing overfitting.<a name=\"six\"></a>[<sup>[6]</sup>](#six) All these trees, when acting together, outperforms any individual tree, a reason we chose this over just a decision tree. When performing classification, it picks the class chosen by the most trees. When performing regression, it takes the averages of all the trees. We plan to use scikit-learn's RandomForestClassifier class in our implementation.\n",
    "\n",
    "**Logistic Regression**: Logistic Regression is a supervised learning algorithm that can be used for classification.<a name=\"seven\"></a>[<sup>[7]</sup>](#seven) While the name is logistic regression, we are planning to use the multinomial version of logistic regression since we have 3 outcomes for the labels(-1, 0, 1) instead of only 2, which is required for regular logistic regression. Since it is only a classifier, the ratings will be used as a categorical variable rather than a continuous variable just for this model. We plan to use scikit-learn's LogisticRegression class in our implementation.\n",
    "\n",
    "**Ordinal Regression**: Ordinal Regression is a supervised learning algorithm with both classification and metric regression properties <a name=\"eight\"></a>[<sup>[8]</sup>](#eight) used to predict ordinal variables or variables whose values are ordered making it an attractive solition for classification problems where the classifiers are ordered such as ours. Because the possible labels our model could predict include negative (-1), neutral (0), and positive (1), they can be considered ordinal as they are dependent in terms of their ordering: negative < neutral < positive. We believe this may improve our model's performance rather than treating the labels as just categorical variables. \n",
    "\n",
    "Each of these aforedescribed models will be used twice, once when the classifiers are labels (ie -1, 0, 1 for negative, neutral, and positive) and once when the classifiers are continuous ratings (1-5). This means that each of them will be performing both classification and regression once each. All of the models can be found in the sklearn Python library, which we will be using.<a name=\"nine\"></a>[<sup>[9]</sup>](#nine)\n",
    "\n",
    "To test the models, we will be initially shuffling the data and splitting it into training, validation, and test sets with a split ratio of 60-20-20. We will train the models using only the training set, and we will use the validation set as an intermediate dataset for evaluating the model’s performance as we tweak parameters. Finally we can use the test set to evaluate each of the model’s ability to generalize and accurately predict on unseen data in accordance with our evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For the first three models – the SVM, Random Forest, and Logistic Regression models used to classify the reviews as negative, neutral, or positive –  we will be using classification accuracy (number of correct predictions divided by the total number of predictions made) to analyze and compare to identify the best model.\n",
    "\n",
    "For the next set of three models –  the SVM, Random Forest, and Logistic Regression used to predict a rating for the reviews on a continuous range between 1 and 5 stars – we will be using root mean squared error (RMSE) to compare the models. The formula for root mean squared error is given by\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum_{i=1}^N (x_i - \\hat{x_i})^2}{N}}$$\n",
    "\n",
    "where $i$ is the index of a datapoint, $N$ is the sample size of the data that is being predicted, $x_i$ is the actual value, and $\\hat{x_i}$ is the predicted value.\n",
    "\n",
    "Because we are using two different metrics to evaluate the classification models and the regression models, we cannot directly compare the performance of the two types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Some potential concerns with data privacy is that in the data itself there are user IDs and usernames that are easily associated with Amazon accounts. In order to address the issue of the data being attached to user IDs and usernames, we will be dropping those columns as they are also unnecessary to our analysis. The dataset also has a column meant to id each review, so we can use that column to identify a given review.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<a name=\"one\"></a>1.[^](#one): Introduction to sentiment analysis: What is sentiment analysis? DataRobot AI Cloud. (2022, March 24). Retrieved April 24, 2022, from https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/<br> \n",
    "<a name=\"two\"></a>2.[^](#two): read, A. min, Finn Bartram·March 5, 2021·238 views, & views, 9 min read·15381. (2022, April 11). Is sentiment analysis machine learning? The CX Lead. Retrieved April 24, 2022, from https://thecxlead.com/topics/is-sentiment-analysis-machine-learnin g/\n",
    "<a name=\"three\"></a>3.[^](#three): Sentiment Analysis & Machine Learning. MonkeyLearn Blog. (2020, April 20). Retrieved April 24, 2022, from https://monkeylearn.com/blog/sentiment-analysis-machine-learning/\n",
    " <br> <a name=\"four\"></a>4.[^](#four): Sentiment analysis. Papers With Code. (n.d.). Retrieved April 24, 2022, from https://paperswithcode.com/task/sentiment-analysis\n",
    " <br> \n",
    "<a name=\"five\"></a>5.[^](#five): Reddy, V. (2020, June 23). Sentiment analysis using SVM. Medium. Retrieved April 24, 2022, from https://medium.com/@vasista/sentiment-analysis-using-svm-338d418e3ff1<br>\n",
    "<a name=\"six\"></a>6.[^](#six): Yiu, T. (2021, September 29). Understanding random forest. Medium. Retrieved April 24, 2022, from https://towardsdatascience.com/understanding-random-forest-58381e0602d2<br>\n",
    "<a name=\"seven\"></a>7.[^](#seven): Swaminathan, S. (2019, January 18). Logistic regression - detailed overview. Medium. Retrieved April 24, 2022, from https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303 <br>\n",
    "<a name=\"eight\"></a>8.[^](#eight): S. E. Saad and J. Yang, \"Twitter Sentiment Analysis Based on Ordinal Regression,\" in IEEE Access, vol. 7, pp. 163677-163685, 2019, doi: 10.1109/ACCESS.2019.2952127. Retrieved May 17, 2022, from https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894084 <br> \n",
    "<a name=\"nine\"></a>9.[^](#nine): Sucky, R. N. (2021, August 28). A complete sentiment analysis project using Python's Scikit-Learn. Medium. Retrieved April 24, 2022, from https://towardsdatascience.com/a-complete-sentiment-analysis-project-using-pythons-scikit-learn-b9ccbb040c2 <br> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
