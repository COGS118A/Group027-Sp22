{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Amy Hardy\n",
    "- Dina Dehaini\n",
    "- Stephanie Kwan\n",
    "- Darren Wu\n",
    "- Nicholas Hale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Sentiment analysis can help business owners process huge amounts of feedback from reviewers and automatically tag them. In our project, we will implement sentiment analysis on over 500k Amazon reviews and attempt to accurately tag/predict reviews based on the actual text of the review. Labels will be derived from their ratings with ratings of 4-5 being positive, 3 neutral, and 1-2 negative. We will also attempt the regression task of predicting a rating between the continuous range of 1 and 5 stars using the review.  We will preprocess the text data through the typical means of removing stop words, stemming, lemmatization, tokenization, and vectorization. Next, an SVM, random forest classifier, and logistic regression model will be trained on the extracted features and evaluated on the remaining data with performance being measured through each model's accuracy in predicting both the labels and the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "\n",
    "Sentiment Analysis is a powerful method for corporations and other organizations to analyze text for a variety of purposes. Sentiment analysis can be defined as the procedure of utilizing natural language processing, text analysis, and other metrics to determine the tonality of a body of text.<a name=\"one\"></a>[<sup>[1]</sup>](#one) When utilized normally, the polarity of analyzed text is separated into categorizations, such as ‘Positive’, ‘Negative’, and ‘Neutral’<a name=\"four\"></a>[<sup>[4]</sup>](#four).\n",
    "\n",
    "The specific manners in which sentiment analysis can be utilized varies along a great range.\tSentiment analysis allows for a consistent metric to analyze text, and in the context of something such as product reviews, allows for the removal of unconscious bias in its analysis in addition to allowing for greater amounts of data to be processed. In a corporate context, sentiment analysis can be utilized to gain realtime insights into consumer opinions and satisfaction and allow for trends to be identified by a text’s polarity.\n",
    "\n",
    "In the context of machine learning, there are a number of algorithms that can be used to analyze text, from Naive Bayes, Linear Regression, Support Vector Machines, neural networks, and more.<a name=\"two\"></a>[<sup>[2]</sup>](#two)\n",
    "\n",
    "Each model has challenges with their implementations, and there are general challenges with sentiment analysis itself in its current state. Text is very nuanced, and there are certain aspects of tone, sentence structure, slang, subjectivity, and more that cause for necessity of improvements upon current models to allow this tool to improve in accuracy.<a name=\"three\"></a>[<sup>[3]</sup>](#three) With further research into this topic, it is possible to identify specific algorithms that allow for the most accuracy when performing sentiment analysis. Performing sentiment analysis on the same dataset multiple times while varying the models used will allow for one to identify which model provides the greatest accuracy for this specific type of data. Product reviews invite different language use by those submitting them than other forms of text, and as mentioned before, performing sentiment analysis on this form of data is largely beneficial to corporations and other parties that may be interested in consumer feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we will be tackling in this project is how to best classify reviews in terms of positive, neutral, and negative as well as in terms of rating. We want to compare how these two different kinds of outputs will perform, and to see which method will produce a better accuracy. We will implement an SVM, random forest classifier, and logistic regression based sentiment analysis model. Specifically, each model will take as input several training reviews and their ratings and then test its accuracy against several test reviews and their ratings by classifying the polarity of a given Amazon review and automatically tag it as positive, neutral, or negative, and also see if we can then give the review a rating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The dataset can be found at https://www.kaggle.com/datasets/jillanisofttech/amazon-product-reviews.\n",
    " \n",
    "This dataset has a total of 568k customer reviews. Each review includes the following fields:\n",
    "- ProductID: the id of the Amazon product\n",
    "- UserID: the id of the Amazon user who is reviewing the product\n",
    "- ProfileName: the profile name of the user who is reviewing the product\n",
    "- HelpfulnessNumerator: number of people who found the review helpful\n",
    "- HelpfulnessDenominator: number of people who did not find the review helpful\n",
    "- Score: the rating the user is giving the product, on a scale of 1-5 stars\n",
    "- Time: the timestamp that the review was posted at\n",
    "- Summary: a short phrase that summarizes the review\n",
    "- Text: the actual text of the review\n",
    "\n",
    "For our project, we will be dropping most features besides the variables that contain text (customer reviews/summary) and rating. The UserID and ProfileName will provide no help in classifying a review’s sentiment, and neither will the timestamp. The values in the helpfulness numerator and denominator often match in each row, which makes their usage and meaning confusing, and overall we believe that the fields will not provide our model additional useful information. We plan to drop the summary field initially as well.\n",
    "\n",
    "An additional column for classification will be made based on the score column –- numbers 1-2 will be -1 for “negative”, 3 will be 0 for “neutral”, and 4-5 will be 1 for “positive” in the new classification column. The reviews and summary will undergo text preprocessing and vectorization before their use in training the models.\n",
    "\n",
    "## Cleaned Data\n",
    "Due to the size of the original CSV file being ~300MB, we had to split it into 4 separate, smaller CSV files so that we could use GitHub. We then wrote a function to retrieve and merge these files together and return the completed dataset.\n",
    "\n",
    "## EDA\n",
    "\n",
    "Taking a closer look at the data, we can see it's heavily skewed and we have a disportionate number of positive reviews. This may impact our model's ability to accurately predict negative review or neutral reviews. This in turn could lead to a higher true positive rate but also a higher false positive rate meaning we'd have a more sensitive and specific model. \n",
    "\n",
    "We also used wordclouds to look at some of the more frequent words based on their sentiment label for both the \"Review\" and \"Summary\" columns. From these plots we can see another potential issue: our model may start to associate neutral words with positive or negative sentiments. Because the products these reviews come from have a star rating and many of the reviews come from these same products, we are left with many reviews for the same product in the same sentiment category. Of course many of these reviews also mention the name of the product (i.e. dog food) and so these words become the most frequent in this sentiment category thus creating an issue for us. Unfortunately, product names were not included as a feature so dropping them from the reviews or summaries may not be possible. Looking at the wordclouds for the summaries, we notice slightly better results in that frequent words are not as neutral as words from the reviews themselves. There are still product names especially in the negative summaries but overall, models trained on the summaries may be able to generalize a little better than models trained on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merge datasets in the data folder.\n",
    "Returns a dataframe of all the data.\n",
    "'''\n",
    "def retrieve_reviews_df():\n",
    "    all_files = glob.glob(\"./data/*.csv\")\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        print(f\"Concatenating {filename}\")\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = retrieve_reviews_df()\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of null values per column\n",
    "reviews_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for the dataframe \n",
    "def score(x):\n",
    "    if x<3:\n",
    "        return -1\n",
    "    elif x==3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def preprocessing(df):\n",
    "    # drop columns\n",
    "    new_df = df.drop(columns = [\"ProductId\", \"UserId\", \"ProfileName\", \"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Time\"])\n",
    "\n",
    "    # drop na values\n",
    "    new_df = new_df.dropna(axis=0)\n",
    "\n",
    "    # make a new column of sentiment: (-1/0/1) -- pos/neutral/neg -- 1,2/3/4,5\n",
    "    new_df['Sentiment'] = new_df.apply(lambda x: score(x['Score']), axis=1)\n",
    "\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing(reviews_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Before feeding the data to any of our models, we will preprocess and vectorize our dataset using scikit-learn's CountVectorizer and TfidfVectorizer classes.\n",
    "\n",
    "**Support Vector Machine**: SVM is a supervised learning algorithm that can be used for both classification and regression. The algorithm is based on identifying a hyperplane that best separates the data into classes and maximizes the separation boundaries between the data points.<a name=\"five\"></a>[<sup>[5]</sup>](#five) The hyperplane is constructed by transforming the data with kernels, which can be linear, sigmoid, polynomial, etc. Because SVM in its simplest form supports binary classification, we will be implementing multiclass SVM (either one vs. one, one vs. all, or directed acyclic graph approach). These involve breaking down the classification into multiple binary classification cases. We plan to use scikit-learn's SVM class in our implementation.\n",
    "\n",
    "**Random Forest Classifier**: RFC is a supervised learning algorithm that can be used for both classification and regression. The algorithm is based on creating several decision trees, which helps in preventing overfitting.<a name=\"six\"></a>[<sup>[6]</sup>](#six) All these trees, when acting together, outperforms any individual tree, a reason we chose this over just a decision tree. When performing classification, it picks the class chosen by the most trees. When performing regression, it takes the averages of all the trees. We plan to use scikit-learn's RandomForestClassifier class in our implementation.\n",
    "\n",
    "**Logistic Regression**: Logistic Regression is a supervised learning algorithm that can be used for classification.<a name=\"seven\"></a>[<sup>[7]</sup>](#seven) While the name is logistic regression, we are planning to use the multinomial version of logistic regression since we have 3 outcomes for the labels(-1, 0, 1) instead of only 2, which is required for regular logistic regression. Since it is only a classifier, the ratings will be used as a categorical variable rather than a continuous variable just for this model. We plan to use scikit-learn's LogisticRegression class in our implementation.\n",
    "\n",
    "**Ordinal Regression**: Ordinal Regression is a supervised learning algorithm with both classification and metric regression properties <a name=\"eight\"></a>[<sup>[8]</sup>](#eight) used to predict ordinal variables or variables whose values are ordered making it an attractive solition for classification problems where the classifiers are ordered such as ours. Because the possible labels our model could predict include negative (-1), neutral (0), and positive (1), they can be considered ordinal as they are dependent in terms of their ordering: negative < neutral < positive. We believe this may improve our model's performance rather than treating the labels as just categorical variables. \n",
    "\n",
    "Each of these aforedescribed models will be used twice, once when the classifiers are labels (ie -1, 0, 1 for negative, neutral, and positive) and once when the classifiers are continuous ratings (1-5). This means that each of them will be performing both classification and regression once each. All of the models can be found in the sklearn Python library, which we will be using.<a name=\"nine\"></a>[<sup>[9]</sup>](#nine)\n",
    "\n",
    "To test the models, we will be initially shuffling the data and splitting it into training, validation, and test sets with a split ratio of 60-20-20. We will train the models using only the training set, and we will use the validation set as an intermediate dataset for evaluating the model’s performance as we tweak parameters. Finally we can use the test set to evaluate each of the model’s ability to generalize and accurately predict on unseen data in accordance with our evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For the first three models – the SVM, Random Forest, and Logistic Regression models used to classify the reviews as negative, neutral, or positive –  we will be using classification accuracy (number of correct predictions divided by the total number of predictions made) to analyze and compare to identify the best model.\n",
    "\n",
    "For the next set of three models –  the SVM, Random Forest, and Logistic Regression used to predict a rating for the reviews on a continuous range between 1 and 5 stars – we will be using root mean squared error (RMSE) to compare the models. The formula for root mean squared error is given by\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum_{i=1}^N (x_i - \\hat{x_i})^2}{N}}$$\n",
    "\n",
    "where $i$ is the index of a datapoint, $N$ is the sample size of the data that is being predicted, $x_i$ is the actual value, and $\\hat{x_i}$ is the predicted value.\n",
    "\n",
    "Because we are using two different metrics to evaluate the classification models and the regression models, we cannot directly compare the performance of the two types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "More code and detailed outputs can be found in our models directory. In this section, we summarize the values for corresponding metrics that we have obtained so far.\n",
    "\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "We ran logistic regression on the processed and vectorized dataset, specifically the \"Reviews\" column to establish our baseline score. We were able to obtain 80.41359% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logReg_pipe = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), \n",
    "                            (\"clf_logReg\", LogisticRegression(multi_class='multinomial',solver ='newton-cg'))])\n",
    "                            \n",
    "clf_logReg_pipe.fit(X_train, y_train)\n",
    "predictedLogReg = clf_logReg_pipe.predict(X_test)\n",
    "np.mean(predictedLogReg == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**\n",
    "\n",
    "We also performed random forest classification on the processed and vectorized data. With this model, we obtained an accuracy of 80.31332%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_randomForest_pipe = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), (\"clf_randomForest\", RandomForestClassifier(verbose=2))])\n",
    "clf_randomForest_pipe.fit(X_train, y_train)\n",
    "predictedRandomForest = clf_randomForest_pipe.predict(X_test)\n",
    "np.mean(predictedRandomForest == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine**\n",
    "\n",
    "Linear support vector classification resulted in 82.10158% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_linearSVC_pipe = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), (\"clf_linearSVC\", LinearSVC(verbose=True))])\n",
    "clf_linearSVC_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these preliminary results from the models, it appears that classifying the data through SVMs results in the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy\n",
    "\n",
    "Some potential concerns with data privacy is that in the data itself there are user IDs and usernames that are easily associated with Amazon accounts. In order to address the issue of the data being attached to user IDs and usernames, we will be dropping those columns as they are also unnecessary to our analysis. The dataset also has a column meant to id each review, so we can use that column to identify a given review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations\n",
    "\n",
    "We will be dividing the work in an as equal manner as possible.\n",
    "- *Expectation 1*: Communicate actively with team members in our Discord group.\n",
    "- *Expectation 2*: Discuss with the team before making big implementation decisions or changes.\n",
    "- *Expectation 3*: Meet at least once a week to complete our task for the week as presented by the project timeline proposal.\n",
    "- *Expectation 4*: Actively participate and contribute to the project.\n",
    "- *Expectation 5*: Meet the day of or the day before a deadline in order to ensure everything is complete to our standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 4/22  | 1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 4/24  | 10:00 PM |  Do background research on topic | Edit, finalize, and submit project proposal | \n",
    "| 5/11  | 7:45 PM | Looking at dataset | Downloading dataset, cleaning dataset, and beginning some preliminary exploration |\n",
    "| 5/12  | 4 PM | Reviewing previous meeting results | Perform EDA, prepare data for use, wrote some further EDA ideas |\n",
    "| 5/16  | 6 PM | Import & Wrangle Data; Do some EDA | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 5/22  | 12 PM | Finalize wrangling/EDA; Begin programming for project | Discuss/edit project code; Complete project |\n",
    "| 5/29  | 12 PM | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project |\n",
    "| 6/8  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<a name=\"one\"></a>1.[^](#one): Introduction to sentiment analysis: What is sentiment analysis? DataRobot AI Cloud. (2022, March 24). Retrieved April 24, 2022, from https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/<br> \n",
    "<a name=\"two\"></a>2.[^](#two): read, A. min, Finn Bartram·March 5, 2021·238 views, & views, 9 min read·15381. (2022, April 11). Is sentiment analysis machine learning? The CX Lead. Retrieved April 24, 2022, from https://thecxlead.com/topics/is-sentiment-analysis-machine-learnin g/\n",
    "<a name=\"three\"></a>3.[^](#three): Sentiment Analysis & Machine Learning. MonkeyLearn Blog. (2020, April 20). Retrieved April 24, 2022, from https://monkeylearn.com/blog/sentiment-analysis-machine-learning/\n",
    " <br> <a name=\"four\"></a>4.[^](#four): Sentiment analysis. Papers With Code. (n.d.). Retrieved April 24, 2022, from https://paperswithcode.com/task/sentiment-analysis\n",
    " <br> \n",
    "<a name=\"five\"></a>5.[^](#five): Reddy, V. (2020, June 23). Sentiment analysis using SVM. Medium. Retrieved April 24, 2022, from https://medium.com/@vasista/sentiment-analysis-using-svm-338d418e3ff1<br>\n",
    "<a name=\"six\"></a>6.[^](#six): Yiu, T. (2021, September 29). Understanding random forest. Medium. Retrieved April 24, 2022, from https://towardsdatascience.com/understanding-random-forest-58381e0602d2<br>\n",
    "<a name=\"seven\"></a>7.[^](#seven): Swaminathan, S. (2019, January 18). Logistic regression - detailed overview. Medium. Retrieved April 24, 2022, from https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303 <br>\n",
    "<a name=\"eight\"></a>8.[^](#eight): S. E. Saad and J. Yang, \"Twitter Sentiment Analysis Based on Ordinal Regression,\" in IEEE Access, vol. 7, pp. 163677-163685, 2019, doi: 10.1109/ACCESS.2019.2952127. Retrieved May 17, 2022, from https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894084 <br> \n",
    "<a name=\"nine\"></a>9.[^](#nine): Sucky, R. N. (2021, August 28). A complete sentiment analysis project using Python's Scikit-Learn. Medium. Retrieved April 24, 2022, from https://towardsdatascience.com/a-complete-sentiment-analysis-project-using-pythons-scikit-learn-b9ccbb040c2 <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
